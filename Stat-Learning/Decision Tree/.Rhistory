knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
fit4<-prune.misclass(fit2,best=9)
fit4
plot(fit4)
text(fit4,pretty=0,cex=0.8)
pred2<-predict(fit4,test,type="class")
pred2
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
train1
fit6<-tree(medv~.,Boston,subset = train1)
fit6
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
View(Boston)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
fit4<-prune.misclass(fit2,best=9)#best越大，也即要的分类树的终端结点个数越多，剪枝后的树会更大，分类的准确率会更低
plot(fit4)
text(fit4,pretty=0,cex=0.8)
pred2<-predict(fit4,test,type="class")
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
pred<-predict(fit6,Boston[-train1,])
pred
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)#计算测试均方误差
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
pred<-predict(fit6,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)#计算测试均方误差
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
fit4<-prune.misclass(fit2,best=9)#best越大，也即要的分类树的终端结点个数越多，剪枝后的树会更大，分类的准确率会更低
plot(fit4)
text(fit4,pretty=0,cex=0.8)
pred2<-predict(fit4,test,type="class")
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
prune.boston = prune.tree(fit6,best = 5)
prune.boston
plot(prune.boston)
text(prune.boston,pretty = 0)
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
pred<-predict(fit6,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)
35.28688^(1/2)
set.seed(3)
fit7<-cv.tree(fit6)
plot(fit7$size,fit7$dev,type="b",main="错误率关于size的变化")
fit8<-prune.tree(fit6,best=5)
plot(fit8)
text(fit8,pretty=0)
library(randomForest)
library(randomForest)
set.seed(1)
fit61<-randomForest(medv~.,Boston,subset = train1,mtry=13,importance=TRUE)
ncol(Boston)
fit61
pred1<-predict(fit61,Boston[-train1,])
pred1
plot(pred1,Boston[-train1,"medv"])
abline(0,1)
mean((pred1-Boston[-train1,"medv"])^2)#计算测试均方误差
summary(prune.boston)
mean((pred2-Boston[-train1,"medv"])^2)
pred2
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
fit4<-prune.misclass(fit2,best=9)#best越大，也即要的分类树的终端结点个数越多，剪枝后的树会更大，分类的准确率会更低
plot(fit4)
text(fit4,pretty=0,cex=0.8)
pred2<-predict(fit4,test,type="class")
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
pred<-predict(fit6,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)#计算测试均方误差
set.seed(3)
fit7<-cv.tree(fit6)#默认用偏差（deviance）来控制交叉验证和剪枝过程
plot(fit7$size,fit7$dev,type="b",main="错误率关于size的变化")
fit8<-prune.tree(fit6,best=5)
plot(fit8)
text(fit8,pretty=0)
library(randomForest)
set.seed(1)
fit61<-randomForest(medv~.,Boston,subset = train1,mtry=13,importance=TRUE)
#mtry=13即在树的每一个分裂点都要考虑所有预测变量，也就是执行装袋法
pred1<-predict(fit61,Boston[-train1,])
plot(pred1,Boston[-train1,"medv"])
abline(0,1)
mean((pred1-Boston[-train1,"medv"])^2)#计算测试均方误差
library(randomForest)
set.seed(1)
fit61<-randomForest(medv~.,Boston,subset = train1,mtry=13,importance=TRUE)
#mtry=13即在树的每一个分裂点都要考虑所有预测变量，也就是执行装袋法
pred1<-predict(fit61,Boston[-train1,])
plot(pred1,Boston[-train1,"medv"])
abline(0,1)
mean((pred1-Boston[-train1,"medv"])^2)#计算测试均方误差
library(randomForest)
set.seed(1)
fit62<-randomForest(medv~.,Boston,subset = train1,mtry=4,importance=TRUE)
#mtry=4，即树上的每个分裂点都只考虑4个预测变量
pred2<-predict(fit62,Boston[-train1,])
plot(pred2,Boston[-train1,"medv"])
abline(0,1)
mean((pred2-Boston[-train1,"medv"])^2)#计算测试均方误差
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
fit4<-prune.misclass(fit2,best=9)#best越大，也即要的分类树的终端结点个数越多，剪枝后的树会更大，分类的准确率会更低
plot(fit4)
text(fit4,pretty=0,cex=0.8)
pred2<-predict(fit4,test,type="class")
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
pred<-predict(fit6,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)#计算测试均方误差
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
plot(fit1)
text(fit1,pretty=0,cex=1)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
fit1
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]
fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")#type = "class"使R返回真实的预测类别
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
fit4<-prune.misclass(fit2,best=9)#best越大，也即要的分类树的终端结点个数越多，剪枝后的树会更大，分类的准确率会更低
plot(fit4)
text(fit4,pretty=0,cex=0.8)
pred2<-predict(fit4,test,type="class")
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)
pred<-predict(fit6,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)#计算测试均方误差
prune.boston = prune.tree(fit6,best = 5)
prune.boston = prune.tree(fit6,best = 5)
pred<-predict(prune.boston,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)
fit0<-randomForest(medv~.,Boston,subset = train1,mtry=13,ntree = 25)
fit0
library(randomForest)
set.seed(1)
fit62<-randomForest(medv~.,Boston,subset = train1,mtry=4,importance=TRUE)
pred2<-predict(fit62,Boston[-train1,])
pred2
plot(pred2,Boston[-train1,"medv"])
abline(0,1)
mean((pred2-Boston[-train1,"medv"])^2)
