---
title: "Untitled"
author: "jessi潘"
date: "2018年3月25日"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
---
#岭回归 
  岭回归一般可以用来解决线性回归模型系数无解的两种情况，一方面是自变量间存在高度多重共线性，另一方面则是自变量个数大于等于观测个数，不管是高度多重共线性的矩阵还是列数多于观测数的矩阵，最终算出来的行列式都等于0或者是近似为0。97年Heer提出了岭回归方法，非常巧妙的化解了这个死胡同，即在X’X的基础上加上一个较小的lambda扰动 ，从而使得行列式不再为0。
![](http://i2.tiimg.com/611786/1a1a64d46b8c5b31.png)

不难发现，回归系数beta的值将随着lambda的变化而变化，当lambda=0时，其就退化为线性回归模型的系数值。
实际上，岭回归系数的解依赖于最优化问题：
![Markdown](http://i2.tiimg.com/611786/1c1bb99761fd5d06.png)

其中最后一项被称为被称为目标函数的惩罚函数，是一个L2范数（(Ridge Regression)）
（即欧氏距离：$$||x||_2= \sqrt{\sum_{i=1}^{n}(x_i)^2}$$，它可以确保岭回归系数beta值不会变的很大，起到收缩的作用，这个收缩力度就可以通过lambda来平衡

对于岭回归来说，随着lambda的增大，模型方差会减小（因为矩阵X’X行列式在增大，其逆就是减小，从而使得岭回归系数在减小）而偏差会增大。故通过lambda来平衡模型的方差和偏差，最终得到比较理想的岭回归系数。
综合来说：
![](http://i2.tiimg.com/611786/32c1c51fb8633a00.png)

岭回归模型可以解决多重共线性的麻烦，正是因为多重共线性的原因，才需要添加这个约束。例如影响一个家庭可支配收入(y)的因素有收入(x1)和支出(x2)，可以根据自变量和因变量的关系构造线性模型：
![](http://i2.tiimg.com/611786/209140d649695947.png)

假如收入(x1)和支出(x2)之间存在高度多重共线性，则两个变量的回归系数之间定会存在相互抵消的作用。即把beta1调整为很大的正数，把beta2调整为很小的负数时，预测出来的y将不会有较大的变化。所以为了压缩beta1和beta2的范围，就需要一个平方和的约束。
将上述等价目标函数展示到几核图形：

![](http://i2.tiimg.com/611786/dc739844fe20a728.png)

![](http://i2.tiimg.com/611786/10c99ac90e81f0fa.jpg)


我们知道岭回归系数会随着lambda的变化而变化，为保证选择出最佳的岭回归系数，该如何确定这个lambda值呢？一般我们会选择定性的可视化方法和定量的统计方法。对这种方法作如下说明：

1）绘制不同lambda值与对应的beta值之间的折线图，寻找那个使岭回归系数趋于稳定的lambda值；同时与OLS相比，得到的回归系数更符合实际意义；

2）方差膨胀因子法，通过选择最佳的lambda值，使得所有方差膨胀因子不超过10（方差膨胀因子(Variance Inflation Factor，VIF):是指解释变量之间存在多重共线性时的方差与不存在多重共线性时的方差之比）；

3）虽然lambda的增大，会导致残差平方和的增加，需要选择一个lambda值，使得残差平方和趋于稳定（即增加幅度细微）。

