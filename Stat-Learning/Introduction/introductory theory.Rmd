---
title: "introductory theory"
author: "陈洁"
date: "2019年5月29日"
output:
  html_document: default
  pdf_document: default
---



#  第一章 导论


##  1.1统计学习概述##
统计学习是一套以理解数据为目的的庞大工具集。统计学习的工具可分为两大类:有指导的学习和无指导的学习。一般而言，有指导的统计学习工具主要有两种用途:一是面向预测的统计模型的建立，二是
对一个或多个给定的输入估计某个输出。在无指导的统计学习问题中，有输入变量但不指定输出变量，建模的主旨是学习数据的关系和结构。



###  1.1.1工资数据
![Markdown](http://i2.tiimg.com/611786/bc1b48d60ec9be4c.png)


观察图片，左图绘制的是每个人的wage与age的关系图，wage随员工age的增长而递增，大约60岁后wage呈明显的下降趋势,曲线是每个age点处员工的平均wage估计，因为wage还与其他因素有关，仅使用age作为唯一的预测变量则不太可能给出某个人wage的精准预测。中图和右图则是wage与每个雇员的educatio以及能挣wage的year的图片，两图都反映这两个因素与wage有关。所以，要想准确地预测一个人的wage，应该结合这个人的年龄age、education和year来分析。


### 1.1.2金融市场数据
![Markdown](http://i2.tiimg.com/611786/ccf5fad1ab72aa10.png)

Wage数据中的输出变量数据类型是连续型，也称为定量型。这类问题通常称为回归(regression) 问题。但在某些情况下，我们可能需要预测一个非数值变量，即分类或定性的输出。如图，我们将考察一个股票市场数据集，该数据集收集了2001年至2005年间年期的标准普尔500股票指数每日股指变动数据。这个数据的目标是用过去5天指数的变动比例预测5天后股指的涨跌状态。这个问题不是对数值类型的变量预测，而是关注某一天的股市业绩是掉进Up桶还是Down桶。这类问题称为分类(classification) 问题。

左侧的两种箱线图分别是股指相对于前一天的百分比变化情况:648天市场股指上涨的分布和602天市场股指下降的情况，两图无明显不同，说明只是用前一天S&P股指变动很难产生预测策略。另外两张箱线图分别显示了用2天前和3天前股指百分比变化预测当期，同样未能反映出过去和当期回报之间的关联性。



### 1.1.3基因表达数据
![Markdown](http://i2.tiimg.com/611786/f43bbd7130e9abd7.png)


上述两个应用的数据集中既有输入变量也有输出变量,而这份数据中只有输入变量却没有相应的输出变量。如左图，这64个细胞系可以通过两个变量$Z_{1}和Z_{2}$来表达，这两个变量是数据中的前两个主成分(principal component)，综合了各个细胞系830个基因表达测量信息。这就是一个聚类(clustering)问题。观察左图，至少说明数据中存在4个细胞系组，图中以不同颜色区分。右图数据的表示方式与左图同理，对14种不同的癌症使用了不同的颜色标记出来。图上清晰地表明同类型癌症的细胞系在这两个特定的二维坐标系上位置较为接近。综上所述，即使在没有癌症信息参与的情况下，左图的聚类结果依然能够将右图中实际的癌症类型所蕴含的相似特征诠释出来。这个例子反映出聚类区别于有指导方法逼近真实的分析能力。




## 1.2统计学习简史
统计学习是一个比较新的名词，但该领域中的许多方法实际上很早以前就已成型。


1. 早在 19 世纪初期，勒让德(Legendre)和高斯(Gauss)在发表有关最小二乘法的文章时，就己提出了今天大家所熟知的线性回归的最早形式。  
2. 为了预测定性变量，如病人存活或死亡，股市涨或跌，费舍尔(Fisher)于1936年提出线性判别分析。
3. 20世纪40年代，许多学者之后又提出了替代的方法——逻里辑斯谛帝回归。
4. 在20世纪70年代初，内尔德(Nelder)和韦德伯恩(Weddderburn)提出了一个新概念——广义线性模型,囊括了统计学习方法中的线性和逻辑斯谛回归在内的线性模型。
5. 到了20世纪70年代末，许多从数据中学习模型的技术获得发展。不过当时主要还是以线性方法为主，因为拟合非线性关系的计算条件当时还尚不成熟。
6. 20世纪80年代，计算技术条件具备，非线性模型不再受计算的困扰。
7. 80年代中期，布赖曼(Breiman)、弗里德曼(Fried-man)、奥申(Olshen)和斯通(Stone)提出了分类回归树，率先示范了一个实用方法在具体实施中的威力，其中包括了用于模型选择的交叉验证法。
8. 1986年，哈斯帖(Hastie)和提布施瓦尼(Tibshirani)提出了一个被称为广义可加模型的概念，将一类非线性模型扩展至广义线性模型族中，还开发了实用软件实现模型。
9. 自20世纪80年代中后期以来，受机器学习(machinelearning)及相关学科影响，统计学习已发展为一个全新的统计学分支，重点关注有指导和无指导的建模和预测。
最近几年，统计学习发展的标志推出了功能越来越强大且用户界面相对友好的软件，比如现在正在流行而且免费的R系统。



## 1.3关于这本书
2001年由哈斯帖(hastie)、提布施瓦尼(Tibshirani)和弗里德曼(Friedman) 编著了《统计学习基础》。该书自出版之日起，很长一段时间以来一直被誉为统计机器学习的奠基之作。《统计学习导论》一书的写作目的在于加速统计学习从学术围向主流领域的融合。无论是在选材的数量上，还是在方法的深度或是在内容表述的细节方面，ISL都不打算取代ESL。我们认为ESL更适用于专业人士(适用于统计、机器学习或相关领域的研究生)，他们需要了解统计学习方法背后的技术细节。然而统计学习技术的用户社区已扩展至个体用户，他们有着更广泛的兴趣和背景。因此，我们相信他们现在正需要这样一方土壤——另一个版本的 ESL，可以不那么注重技巧却更引人入胜。
本书的写作意图是基于以下4方面的认识：


1.许多统计学习方法不仅仅属于统计学科，而是在许多学术和非学术的领域里有广泛的应用。

2.统计学习并非一纽黑箱。

3.尽管掌握组合模型中每个齿轮的功能至关重要，但这并不表示没有掌握理论细节就不能在箱子里构造机器!

4.假定读者有志于应用统计学习方法解决现实世界的问题。








## 1.4记号与简单的矩阵代数
记号中，n表示不同数据点或样本观测的个数；p表示用于预测的变量个数。一般地，$x_{ij}代表第i个观测的第j个变量值,i = 1,2,{\ldots}, n;j = 1,{\ldots},p。在本书中i用于索引样本或观测（从1到n），j用于索引变量（从1到p）。\bf{X}则是(i,j)上元素为x_{ij}的n\times{p}$矩阵即

$$
        {X}=\begin{pmatrix}
        x_{11} & x_{12} & \cdots & x_{1p} \\
        x_{21} & x_{22} & \cdots & x_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \cdots & x_{np} \\
        \end{pmatrix}
$$
对矩阵不熟悉的读者，可将$\bf{x}$看成$n$行$p$列的表格。
有时会对$\bf{X}$的行进行研究，通常将行记为$x_1,x_2,{\ldots},x_n$。这里$x_i$是长度为$p$的向量，是第$i$个观测的$p$个变量测量，即
$$
        {x_i}=\begin{pmatrix}
        x_{i1} \\
        x_{i2} \\
        \vdots \\
        x_{ip} \\
        \end{pmatrix}
$$
（向量默认用列表示。）例如，对于 Wage数据集，$x_i$是一个长度为 12 的向量，包含第$i$个个体的year(年份)、age(年龄)、wage(工资)和其他变量值。有时也会研究$\bf{X}$的列，将其写作$x_1,x_2\cdots\ x_p$。它们都是长度为$n$的向量，即
$$
        {X_j}=\begin{pmatrix}
        x_{1j} \\
        x_{2j} \\
        \vdots \\
        x_{nj} \\
        \end{pmatrix}
$$
例如，Wage数据集中，$x_1$包含year（年份）的n=3000个观测值。
按这个记号，矩阵$\bf{X}$可以写成
           $$\bf{X}=(x_1\ x_2\cdots\ x_p)$$
或
$$
        {X}=\begin{pmatrix}
        x_{1}^T \\
        x_{2}^T \\
        \vdots \\
        x_{n}^T \\
        \end{pmatrix}
$$

右上角记号$T$表示矩阵或向量的转置。例如${x_i}^T=(x_{i1}\ x_{i2}\cdots\ x_{ip})$,以及
$$
        {X}^T=\begin{pmatrix}
        x_{11} & x_{21} & \cdots & x_{n1} \\
        x_{12} & x_{22} & \cdots & x_{n2} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{1p} & x_{2p} & \cdots & x_{np} \\
        \end{pmatrix}
$$
$y_i$表示待预测的变量(比如wage) 的第$i$个观测值。待预测变量全部 $n$个观测值的集合用如下向量表示:
$$
        y=\begin{pmatrix}
        y_{1} \\
        y_{2} \\
        \vdots \\
        y_{n} \\
        \end{pmatrix}
$$
观测数据集为$\lbrace(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\rbrace$，其中$x_i$都是长度为$p$的向量。（若$p=1$，则$x_i$是标量。）
在本书中，长度为$n$的向量均用小写加粗字母表示，例如：
$$
        \bf a=\begin{pmatrix}
        a_1 \\
        a_2 \\
        \vdots \\
        a_n \\
        \end{pmatrix}
$$
但长度不为$n$的向量则用小写常规字母表示，例如$a$。标量也用小写常规字母表示，例如$a$。在极少数情况下，小写常规字母的两种不同用了两种不同用法会导致含义不明，当出现这一问题时，书中会加以说明。矩阵用加粗大写字母表示，例如$\bf{A}$。随机变量不论维数，一律用大写常规字母表示，例如$A$。
有时需要指出对象的维数。记号$a\in{\Bbb R}$说明某个对象是标量（若长度为$n$,则用$a\in{\Bbb R}^n$表示）。$a\in{\Bbb R}^k$说明某对象为向量，其长度为$k$。$\bf{A}\in{\Bbb R}^{r\times s}$则说明矩阵的维数是$r\times s$。
我们尽可能地避免矩阵代数的使用。但在少数情况下，完全回避矩阵代数会使计算变得缓慢冗长。在这些情况下，理解矩阵乘法的概念是很重要的。$\bf{A}\in{\Bbb R}^{r\times d}$，$\bf{B}\in{\Bbb R}^{d\times s}$。则$\bf{A}$与$\bf{B}$相乘的结果记为$\bf{AB}$。矩阵$\bf{AB}$的第$(i,j)$个元素等于$\bf{A}$中的第$i$行和$\bf{B}$中的第$j$列的对应元素乘积之和。即$(\bf{AB})_{ij}=\sum_{k=1}^d a_{ik}b_{kj}$。

## 1.5本书的内容安排


第2章主要介绍统计学习的基本技术和概念，这章还包括了一类原理简单却在许多领域运用自如的$\mit{K}$最近邻分类方法。

第3章主要回顾线性回归，这是所有回归方法的基础。

第4章讨论了两类重要的分类模型:逻辑斯谛回归和线性判别分析。

第5章重点介绍交叉验证和自助法，这些方法可通过估计不同方法的精度选择最优的模型。

第6章我们提供了一类集经典与现代于一体的线性模型，这些方法是在标准线性回归基础上的改进，包括逐步变量选择、龄回归、主成分回归、偏最小二乘和lasso 回归。

第7章首先介绍一类在一元输入变量问题中颇有成效的非线性方法，之后将说明这些方法如何被运用到多于一个输入变量的非线性可加模型中。

在第8章中，重点考察树类模型，包括装袋法、提升法和随机森林，支持向量机是一种既可以用于线性分类，也可以用于非线性分类的一组方法，将在第9章中介绍。

 第 10 章考虑只有输入变量没有输出变量的一类方法，重点讲述主成分分析、 $\mit{K}$均值聚类和系统聚类方法。


## 1.6用于实验和习题的数据集
在这本教材中，我们将展现统计学习方法在各个领域的应用，这些领域包括市场营销、金融、生物和其他领域等。ISLR软件包可以从本书的网站下载，那里还有实验和习题用的数据集。其他的数据来自R的MASS库和R的基础数据。表 1-1 中列出了本书实验和习题所需要的数据集清单。读者也可以从该书的网站上下载这些数据集的文本格式，其中一些将在第2章使用。



## 1.7本书网站
     
        
        http://www - bcf. usc. edu/ ~ gareth/ISL/



## 1.8致谢
书中的图6-7、图8-3和图10-12来自ESL其他的图都是本书新增的。



