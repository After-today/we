#第四讲 支持向量机

##1 前言

通常把最大间隔分类器、支持向量分类器和支持向量机都简单叫做“**支持向量机**”，但他们是具有严格的区分的（下面会区分）

支持向量机被认为是适应性最广的分类器之一

---

##2 最大间隔分类器(MMC)

###2.1 超平面

$p$维空间中超平面是$p-1$维的平面仿射子空间，比如说，2维空间中的超平面是$\beta_0+\beta_1X_1+\beta_2X_2=0$,是一条直线（1维）；3维空间中的超平面是$\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3=0$,是一个平面（2维）；$p$维空间中的超平面就是$\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p=0$,这是$p-1$维的。

超平面可以把一个空间分割成两个部分

###2.2 最大间隔超平面

由于分割超平面不唯一，需要找到一个分割超平面，使所有到这个超平面的训练观测的垂直最小距离最大，这样的超平面就叫做最大间隔超平面。比如说下图，有好几条线（分割平面）可以用来分割红、蓝点（左图画出了三种分割平面，当然还有更多），右图标出了所有到**这一个**超平面的训练观测的垂直最小距离（有三个点），以同样的方式标出另外两个超平面与观测的垂直最小距离，可以发现右图所示的超平面有**最大**的**垂直最小距离**。

相当于是这三个点决定了这个最大间隔超平面，如果这三个点的位置有所改变，最大间隔超平面也会改变，所以这三个点起到了某种支持作用，被称作**“支持向量”**。这也意味着，最大间隔超平面只由支持向量决定，与其他向量无关。

最大间隔超平面对单个观测的变化极其敏感，有可能会出现**过拟合**情况

![](C:\Users\yangy\Desktop\zl\svm\project about svm\9.2.png)


###2.3 构建最大间隔分类器

$n$个观测$x_1,x_2,...,x_n$和其对应的类别标签$y_1,y_2,...,y_n\in\{-1,1\}$，假设超平面是$\beta_0+\beta_1x_{i1}+...+\beta_px_{ip}=0$

那么最大间隔分类器就是一个优化问题$\max_{\beta_0,\beta_1,...,\beta_p} M$,

满足$\sum_{j=1}^{p}\beta_j^2=1 (1)\\yi(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip})\ge M,i=1,...,n  (2)$

(1)式的约束使得第$i$个观测到超平面的垂直距离为$yi(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip})$，使得(2)式有意义。(2)式保证了每个观测都在超平面正确的一侧,且与超平面的距离至少为$M$。所以问题就转化为了去找$\beta_0,\beta_1,...,\beta_p$以此最大化$M$，这边通常使用**引入拉格朗日对偶变量+序列最小最优化算法(SMO算法)**来求解（不讲）

通过最大化间隔，使得该分类器对数据分类是具有了最大把握。使用最大间隔而非最小间隔，是因为最大间隔能获得最大稳定性与区分的确信度（离最大间隔越远的观测的分类越具有可信度），从而获得良好的推广能力（超平面的$M$越大，这个分类器的性能越好，推广能力也就越好）


---

##3 支持向量分类器(SVC)

上面讲的都是线性可分的情况，但是大多数情况下，是线性不可分的，比如说下图的情况，就无法用一条直线来分割

![](C:\Users\yangy\Desktop\zl\svm\project about svm\9.4.png)

那么在线性不可分情况下，就要考虑将最大间隔分类器进行推广——>**支持向量分类器**

###3.1 概念

支持向量分类器也叫做软件隔分类器，允许小部分观测误分以保证大部分观测可以被更好地分类，提高分类器的稳定性

观测误分有两种情况：

（1）落在间隔错误的一侧（离超平面太近）

（2）落在超平面错误的一侧（入曹营）

左图的**1、8**两个点，穿过了间隔，落入了间隔错误的一侧，但还是在超平面正确的一侧

右图的**1、8**两个点，穿过了间隔，落入了间隔错误的一侧，但还是在超平面正确的一侧，**11、12**两个点直接穿过了超平面落在了超平面错误的一侧

![](C:\Users\yangy\Desktop\zl\svm\project about svm\9.6.png)

###3.2 构建支持向量分类器

$n$个观测$x_1,x_2,...,x_n$和其对应的类别标签$y_1,y_2,...,y_n\in\{-1,1\}$，假设超平面是$\beta_0+\beta_1x_{i1}+...+\beta_px_{ip}=0$

那么支持向量分类器就是一个优化问题$\max\min_{\beta_0,\beta_1,...,\beta_p,\epsilon_1,\epsilon_2,...,\epsilon_n} M$,

满足$\sum_{j=1}^{p}\beta_j^2=1 (1)\\yi(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip})\ge M(1-\epsilon_i),i=1,...,n  (2)\\\epsilon_i\ge 0,\sum_{i=1}^{n}\epsilon_i\le C (3)$

$\epsilon_1,\epsilon_2,...,\epsilon_n$是松弛变量，使得分类器允许训练观测中有小部分观测可以落在间隔的错误的一侧($\epsilon_i>0$)或是超平面错误的一侧($\epsilon_i>1$)，$C$是非负调节参数，代表了容忍度，$C$越大，则间隔越大，落在间隔错误的一侧的观测就越多。

![](C:\Users\yangy\Desktop\zl\svm\project about svm\9.7.png)

$C$越来越小，间隔越来越小，落在间隔上的观测也越来越少

在支持向量分类器中的支持向量是那些落在间隔上和落在间隔错误一侧的观测，以上图（右）为例，**1、2、7、8、9、11、12**是该分类器的支持向量。当$C$比较大的时候，穿过间隔的观测就多，这意味着支持向量的数量会增加，从而确定该超平面涉及到的观测点就相应地增加了，使得该分类器具有较低的方差和较高的偏差

###3.3 实践

```{r warning=FALSE,message=FALSE}
#先建立线性可分的数据集
set.seed(1)
x = matrix(rnorm(20 * 2), ncol = 2)
y = c(rep(-1, 10), rep(1, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = (3 - y))
dat = data.frame(x = x, y = as.factor(y))
```

```{r warning=FALSE,message=FALSE}
library(e1071)
#由于这边用了线性核函数，不需要去考虑gamma参数
svmfit3.3.1 = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit3.3.1, dat)#“X”是支持向量，“O”是非支持向量
svmfit3.3.1$index#用以确定是哪7个支持向量
summary(svmfit3.3.1)
```

**结果解读：**cost为10的分类器，gamma的值是默认的特征变量的个数的倒数，模型将数据分为两类，有7个支持向量，一类4个，一类3个

```{r warning=FALSE,message=FALSE}
#改变cost值
svmfit3.3.2 = svm(y ~ ., data = dat, kernel = "linear", cost = 0.1, scale = FALSE)
plot(svmfit3.3.2, dat)
svmfit3.3.2$index
```

**结果解读：**该模型的cost相比前一个模型小，显然从图中就可以看到，支持向量比上一个模型多了很多

---

##4 核函数

###4.1 引入核函数

到支持向量分类器为止，还都是线性回归。但是很多时候，预测变量与响应变量之间的关系是非线性的，这个时候再使用线性回归就会使分类结果大打折扣。我们需要使用预测变量的函数来扩大特征空间，将变量映射到一个高维特征空间，以此使其线性可分（如下图）。在引入支持向量机的概念之前，我们先介绍一下核函数。

![](C:\Users\yangy\Desktop\zl\svm\project about svm\9.8.PNG)

原始的扩大特征空间的方法诸如使用二次多项式，三次多项式，可能会导致出现数量庞大的特征，导致**计算量**激增，所以我们引入**核函数**

对于线性学习器，考虑以下函数

$f(x)=\sum_{i=1}^{p}\beta_i \phi_i(x)+\beta_0$

分类函数也就是决策规则可以用测试点和训练点的内积表示(引入了拉格朗日对偶变量$\alpha$后)

$f(x)=\sum_{i=1}^{p}\alpha_iy_i\langle \phi_i(x_i),\phi(x)\rangle+\beta_0$（4.2中解释径向核函数还要提到这个公式）

原始的映射方法建立非线性学习器步骤：

（1）使用一个非线性映射将数据变换到一个特征空间$x->\phi(x)$

（2）在特征空间计算内积$\langle \phi_i(x_i),\phi(x)\rangle$

（3）在特征空间使用线性学习器分类

使用核函数方法可以在特征空间中直接计算内积，也即合并了上面的（1）（2）这两个步骤

下面给出核函数的数学定义：

核是一个函数$\kappa$，对所有$x,z\in \chi$，满足$\kappa(x,z)=\langle\phi(x),\phi(z)\rangle$

使用恰当的核函数来代替内积，可以隐式地将非线性训练数据映射到高维空间，而不增加可调参数的个数

**补充：**任何将计算表示为数据点的内积的方法，都可以用核方法进行非线性扩展

###4.2 几个核函数

下面的（1）（2）（3）（4）是**e1071包的svm()函数**中的四种核函数

**（1）线性核函数**$\kappa(x_i,x_j)=\langle x_i,x_j\rangle$

**（2）多项式核函数**$\kappa(x_i,x_j)=(\gamma\langle x_i,x_j\rangle+R)^d$

适合正交归一化的数据（**向量正交且模为1**），属于全局核函数，允许相距很远的数据点对核函数的值有影响。d越大，映射的维度就越高，计算量也就越大，要注意d过大可能会出现过拟合。

**（3）径向核函数**$\kappa(x_i,x_j)=\exp\big(-\gamma\sum_{m=1}^{p}(x_{im}-x_{jm})^2\big)=\exp\big(-\gamma\lvert x_i-x_j\rvert^2\big)$

高斯核函数$\kappa(x_i,x_j)=\exp\big(-\cfrac{\lvert x_i-x_j\rvert^2}{2\sigma^2}\big)$实际就是径向核函数

对数据中存在的噪声具有良好的抗干扰能力，属于局部和函数，当数据点离中心点变远时，取值会变小，随着gamma的减小（$\sigma^2$的增大），函数的作用范围会变小（gamma决定函数的作用范围）

**补充：**径向核函数是如何起作用的：

如果测试观测$x$离训练观测$x_i$很远，那么径向核函数$\kappa(\phi(x_i),\phi(x))$就会很小，那么在分类函数中，就意味着这个训练观测$x_i$对测试观测$x$几乎没有影响，也就意味着，距离$x$远的训练观测$x_i$对预测$x$的类别几乎没有帮助

**注意**在e1071包的svm()函数中，用的是径向核函数的表达式而非高斯核函数的表达式

**（4）Sigmoid核函数**$\tanh(\gamma\langle x_i,x_j\rangle+R)$（我这边没用过）

来源于神经网络，广泛应用于深度学习和机器学习中，此时，SVM实现的是一种多层感知器神经网络

**（5）字符串核函数**

是定义在字符串集合上的核函数，度量一对字符串的相似度，在文本分类、信息检索方面有应用

svm()函数中有两个参数，gamma和cost
从上述式子来看，（1）是不需要考虑gamma的，即使用系统默认（默认是特征变量个数的倒数）即可，（2）（3）（4）均要考虑gamma参数的选取。cost参数与上面模型中的C的意义（容忍度）不同，意为观测穿过间隔的成本，cost越小，间隔越宽，穿过间隔就更容易，支持向量就越多。这两者对于模型的意义刚好相反。

###4.3 参数详解

**gamma参数**

![](C:\Users\yangy\Desktop\zl\svm\project about svm\9.9.png)

$\gamma$越大，也即$\sigma^2$越小，训练准确率越高。理论上，$\gamma$趋向于无穷大时，可以拟合任何数据，但是对测试集的分类会比较差，具有低偏差，高方差性，这就会有过拟合的问题。而$\gamma$较小时，具有高偏差，低方差性，可能会出现欠拟合问题。

**cost参数**

cost小，穿过间隔的成本小，导致支持向量多，容易出现过拟合，生成较为复杂的边界，具有低偏差，高方差性

cost大，穿过间隔的成本大，导致支持向量少，容易出现欠拟合，生成较为平滑的边界，具有高偏差，低方差性

###4.4 核函数的选择

当特征数》样本数  使用线性核函数

  特征数《样本数  使用径向核函数
  
  特征数和样本数一样都很大  使用线性核函数（因为线性核和径向核的效果差不多，但线性核在效率上更优）

---

##5 支持向量机(SVM)

支持向量机是支持向量分类器的扩展，在支持向量分类器中引入了**核函数**。
```{r warning=FALSE,message=FALSE}
#建立一个线性不可分的数据集
set.seed(1)
x = matrix(rnorm(200 * 2), ncol = 2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2
y = c(rep(1, 150), rep(2, 50))
dat = data.frame(x = x, y = as.factor(y))
plot(x, col = y)
train = sample(200, 100)
svmfit5.1 = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)#这边用了径向核函数，因此除了参数cost，也要规定gamma参数
plot(svmfit5.1, dat[train,])
summary(svmfit5.1)
#改变gamma的值
svmfit5.2 = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 5, cost = 1)
plot(svmfit5.2, dat[train,])
summary(svmfit5.2)

set.seed(1)
#下面这个函数是用来通过交叉验证来选择径向核函数最优的gamma和cost#用tune.svm()也能得到一样的结果
tune.out = tune(svm, y ~ ., data = dat[train,], kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)#这个语句给出了不同cost和gamma值对应的交叉验证误差，而且给出了最好的模型的cost=1，gamma=2
tabsvm<-table(true = dat[-train, "y"], pred = predict(tune.out$best.model, newdata = dat[-train,]))#tune.out$best.model就是使交叉验证误差最小的cost和gamma所建立的分类器
tabsvm
sum(diag(prop.table(tabsvm)))#计算正确分类率
```

---

##6 ROC曲线

```{r warning=FALSE,message=FALSE}
#比较不同gamma取值的ROC曲线
library(ROCR)
rocplot = function(pred, truth, ...) {
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
}
#训练集数据
svmfit.gamma2 = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 2, cost = 1, decision.values = T)
fittedgamma2 = attributes(predict(svmfit.gamma2, dat[train,], decision.values = TRUE))$decision.values
par(mfrow = c(1, 2))
rocplot(fittedgamma2, dat[train, "y"], main = "Training Data")
svmfit.gamma50 = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 50, cost = 1, decision.values = T)
fittedgamma50 = attributes(predict(svmfit.gamma50, dat[train,], decision.values = T))$decision.values
rocplot(fittedgamma50, dat[train, "y"], add = T, col = "red")

#测试集数据
fittedgamma2 = attributes(predict(svmfit.gamma2, dat[-train,], decision.values = T))$decision.values
rocplot(fittedgamma2, dat[-train, "y"], main = "Test Data")
fittedgamma50 = attributes(predict(svmfit.gamma50, dat[-train,], decision.values = T))$decision.values
rocplot(fittedgamma50, dat[-train, "y"], add = T, col = "red")
```

**结果解读：**训练集数据的ROC图显示增加gamma可以更加光滑地拟合数据，但是在测试集数据中，gamma2比gamma50模型能提供更准确的预测结果

---

##7 多分类的SVM

###7.1 一类对一类

构建$\binom{K}{2}$个SVM模型，每个SVM模型可以分隔两个类

具体:使用$\binom{K}{2}$个SVM模型对一个测试观测进行分类，记录这个测试观测被分到每个类别的次数，分到次数最多的这个类，就是这个测试观测的最终预测类别

a,b,c三类

可以做出三个SVM模型：a|b,a|c,b|c

测试观测分到：a,a,b

那么这个测试观测最终被分到类“a”


###7.2 一类对其余

第m个类与其余K-1个类构建SVM模型，这样的话一共有K个SVM模型，把$x^*$分到使$f(x^*)=\beta_{0m}+\beta_{1m}x_1^*+\beta_{2m}x_2^*+...+\beta_{pm}x_p^*$最大的那个类"m"。这意味着把这个测试观测预测的类“m”，而不是其他的类别是具有高度的信心的




---

##8 总结

###8.1 SVM与Logistic回归比较

不同类别的 可以很好地被分离时，SVM表现更好；不同类别存在较多重叠时，Logistic回归表现更好

###8.2 损失函数+惩罚项

支持向量分类器里面的优化问题也可以写成以下形式：
但是这边不再要求$\sum \beta^2=1$，而是要求间隔对应的值是1（这种模式出现在大多数参考书籍里面）

$\min_{\beta_0,\beta_1,...,\beta_p}\big\{\sum_{i=1}^{n}\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^{p}\beta_{j}^{2}\big\}$

这里的$\lambda$相当于是前面的容忍度$C$,$\lambda\sum_{j=1}^{p}\beta_{j}^{2}$是惩罚项，用以控制方差-偏差权衡，$\sum_{i=1}^{n}\max[0,1-y_if(x_i)]$是损失函数（这种损失函数是铰链损失函数），是模型对数据拟合程度的某种量化。如果被正确分类，那么$y_if(x_i)\ge1$，这就意味着$\max[0,1-y_if(x_i)]=0$，损失为0，即在间隔之外的正确分类的观测不会影响分类器，而那些在间隔错误一侧的观测（也就是那些支持向量），使$y_if(x_i)<1$，则$\max[0,1-y_if(x_i)]=1-y_if(x_i)>0$，由于最终的优化目标是min，这与其相悖，因此会影响到分类器的构建

这一part先讲到这里，后面还会深入


