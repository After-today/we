{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 3.4结巴分词\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "三种分词模式 ： \n",
    "A、精确模式：试图将句子最精确地切开，适合文本分析。默认是精确模式。 \n",
    "B、全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义\n",
    "C、搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\n",
    " 注：当指定jieba.cut的参数HMM=True时，就有了新词发现的能力。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "结巴分词算法：https://blog.csdn.net/rav009/article/details/12196623\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\haoyu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.995 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式： 今天|今天天气|天天|天气|真好|||亲爱|的|||我们|去|远足|吧||\n",
      "精确模式： 今天天气|真|好|。|亲爱|的|，|我们|去|远足|吧|！\n",
      "搜索引擎： 今天|天天|天气|今天天气|真|好|。|亲爱|的|，|我们|去|远足|吧|！\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "'''\n",
    "cut方法有两个参数\n",
    "1)第一个参数是我们想分词的字符串\n",
    "2)第二个参数cut_all是用来控制是否采用全模式\n",
    "'''\n",
    "#全模式\n",
    "word_list = jieba.cut(\"今天天气真好。亲爱的，我们去远足吧！\",cut_all=True)\n",
    "print( \"全模式：\",\"|\".join(word_list))\n",
    "#精确模式 , 默认就是精确模式\n",
    "word_list = jieba.cut(\"今天天气真好。亲爱的，我们去远足吧！\",cut_all=False)\n",
    "print (\"精确模式：\",\"|\".join(word_list))\n",
    "#搜索引擎模式\n",
    "word_list = jieba.cut_for_search(\"今天天气真好。亲爱的，我们去远足吧！\")\n",
    "print (\"搜索引擎：\",\"|\".join(word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def read_file(filename):\n",
    "        stop = [line.strip() for line in  open('F:/课程设计及上机实验/python/文本分析/中文分词/Chinese-StopWords.txt','r',encoding='utf-8').readlines()]  #停用词\n",
    "        f = open(filename,'r',encoding='utf-8')\n",
    "        line = f.readline()\n",
    "        str = []\n",
    "        while line:\n",
    "            s = line.split('\\t')\n",
    "            fenci = jieba.cut(s[0],cut_all=False)  #False默认值：精准模式\n",
    "            #word=print( \"|\".join(fenci))\n",
    "            str.append(list(set(fenci)-set(stop)))\n",
    "            line = f.readline()\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = [line.strip() for line in  open('F:/课程设计及上机实验/python/文本分析/中文分词/Chinese-StopWords.txt','r',encoding='utf-8').readlines()]  #停用词\n",
    "f = open('F:/课程设计及上机实验/python/文本分析/中文分词/test/test1.txt','r',encoding='utf-8')\n",
    "line = f.readline()\n",
    "str = []\n",
    "while line:\n",
    "    s = line.split('\\t')#\\t\\r\\n均为转义字符，\\t是横向跳到下一个制表符位置\n",
    "    fenci = jieba.cut(s[0],cut_all=False)  #False默认值：精准模式\n",
    "    str.append(list(set(fenci)-set(stop)))\n",
    "    line = f.readline()\n",
    "print(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import  FreqDist,ConditionalFreqDist\n",
    "from nltk.metrics import  BigramAssocMeasures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取信息量最高(前number个)的特征(卡方统计)\n",
    "nltk中的FreqDist,ConditionalFreqDist：https://blog.csdn.net/zhuzuwei/article/details/79154371 \n",
    "条件频率分布需要处理的是配对列表，每对的形式是（条件，事件），conditions()函数会返回这里的条件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_feature(number): \n",
    "        posWords = []\n",
    "        negWords = []\n",
    "        for items in read_file('F:/课程设计及上机实验/python/文本分析/中文分词/clothing/pos.txt'):\n",
    "            #把集合的集合变成集合\n",
    "            for item in items:\n",
    "                posWords.append(item)\n",
    "        for items in read_file('F:/课程设计及上机实验/python/文本分析/中文分词/clothing/neg.txt'):\n",
    "            for item in items:\n",
    "                negWords.append(item)\n",
    "\n",
    "        word_fd = FreqDist() #可统计所有词的词频\n",
    "        cond_word_fd = ConditionalFreqDist() #可统计积极文本中的词频和消极文本中的词频\n",
    "\n",
    "        for word in posWords:\n",
    "            word_fd[word] += 1\n",
    "            cond_word_fd['pos'][word] += 1\n",
    "\n",
    "        for word in negWords:\n",
    "            word_fd[word] += 1\n",
    "            cond_word_fd['neg'][word] += 1\n",
    "\n",
    "        pos_word_count = cond_word_fd['pos'].N() #积极词的数量\n",
    "        neg_word_count = cond_word_fd['neg'].N() #消极词的数量\n",
    "        total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "        word_scores = {}#包括了每个词和这个词的信息量\n",
    "\n",
    "        for word, freq in word_fd.items():\n",
    "            pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word],\n",
    "                       (freq, pos_word_count),total_word_count) \n",
    "            #计算积极词的卡方统计量，这里也可以计算互信息等其它统计量\n",
    "            neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word],\n",
    "                        (freq, neg_word_count), total_word_count) \n",
    "            word_scores[word] = pos_score + neg_score \n",
    "            #一个词的信息量等于积极卡方统计量加上消极卡方统计量\n",
    "\n",
    "        best_vals = sorted(word_scores.items(), key=lambda item:item[1], reverse=True)[:number] #把词按信息量倒序排序。number是特征的维度，是可以不断调整直至最优的\n",
    "        best_words = set([w for w,s in best_vals])\n",
    "        return dict([(word, True) for word in best_words])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keys()详解：https://www.cnblogs.com/wushuaishuai/p/7738313.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features():\n",
    "     #feature = bag_of_words(text())#第一种：单个词\n",
    "     #feature = bigram(text(),score_fn=BigramAssocMeasures.chi_sq,n=500)#第二种：双词\n",
    "     #feature =  bigram_words(text(),score_fn=BigramAssocMeasures.chi_sq,n=500)#第三种：单个词和双个词\n",
    "        feature = jieba_feature(1000)#第四种：结巴分词\n",
    "\n",
    "        posFeatures = []\n",
    "        for items in read_file('F:/课程设计及上机实验/python/文本分析/中文分词/clothing/pos.txt'):\n",
    "            a = {}\n",
    "            for item in items:\n",
    "                if item in feature.keys():\n",
    "                    a[item]='True'\n",
    "            posWords = [a,'pos'] #为积极文本赋予\"pos\"\n",
    "            posFeatures.append(posWords)\n",
    "        negFeatures = []\n",
    "        for items in read_file('F:/课程设计及上机实验/python/文本分析/中文分词/clothing/neg.txt'):\n",
    "            a = {}\n",
    "            for item in items:\n",
    "                if item in feature.keys():\n",
    "                    a[item]='True'\n",
    "            negWords = [a,'neg'] #为消极文本赋予\"neg\"\n",
    "            negFeatures.append(negWords)\n",
    "        return posFeatures,negFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得训练数据\n",
    "posFeatures,negFeatures =  build_features()\n",
    "posFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(posFeatures) \n",
    "shuffle(negFeatures) #把文本的排列随机化 \n",
    "#N1=int(0.3*total_word_count/2)\n",
    "train =  posFeatures[300:]+negFeatures[300:]#训练集(70%)\n",
    "test = posFeatures[:300]+negFeatures[:300]#验证集(30%)\n",
    "data,tag = zip(*test)#分离测试集合的数据和标签，便于验证和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
