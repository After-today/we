{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 情感分析\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "情感分析：又称为倾向性分析和意见挖掘，它是对带有情感色彩的主观性\n",
    " 文本进行分析、处理、归纳和推理的过程，其中情感分析还可以细分为情\n",
    " 感极性（倾向）分析，情感程度分析，主客观分析等。\n",
    " 情感极性分析的目的是对文本进行褒义、贬义、中性的判断。在大多应用\n",
    " 场景下，只分为两类。例如对于“喜爱”和“厌恶”这两个词，就属于不同\n",
    " 的情感倾向。\n",
    " 背景交代：下载CSDN上的一个中文情感分析语料库，包含 酒店、服装、水果、\n",
    " 平板、洗发水 等 5 个领域的评价数据，每个领域各包含 5000 条正面和负面\n",
    " 评价，数据抓取于携程网和京东。本文选取服装的评论数据进行情感分析。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "情感分析语料库：https://download.csdn.net/download/u010097581/9919245 \n",
    "基于pyhton的情感分析案例：https://blog.csdn.net/yawei_liu1688/article/details/79011697\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "示例1（好评）： 质量好,做工也不错,尺码标准,\n",
    "裤子质量很好，裤型不错，而且穿起来显瘦，性价比高，是我喜欢的布料，不起球，值得购买，\n",
    " 做工很好，货真价实，质量不错满意！\n",
    " 真是不错呢！裤子质量很好，穿着也很有型，帅帅的，更喜欢他了呢！快递很快，态度也特别好，\n",
    " 跟客服咨询问题能够得到快快速的回答，下次还会光顾，么么哒\n",
    " 品质还可以，就是有点掉色，可以接受\n",
    " 质量不错，尺码大小合适，很喜欢\n",
    " 很喜欢，很满意，下次我还会再来的，不会掉色，质量很不错的\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "示例2（差评）：\n",
    " 穿上不舒服，颜色和质感跟图片差异很大，建议慎重购买！后悔了！\n",
    " 真心垃圾，以后再也不会买班尼路了\n",
    " 物流超慢，整整6天多，准备退订了才到，无语。衣服质量嘛，中下\n",
    " 水准，穿着不太舒服。衣服穿起来的外形跟农民工差不多，设计太垃\n",
    " 圾了。总之不值这个价，三分之一的价格买来都觉得亏。谁买谁哭！\n",
    " 这个T恤不安逸得，和农贸市场一二十的差不多样\n",
    " 你以为是Lee那就错了，你以为会有Lee一半的品质，那就又错了，\n",
    " 你以为会送货很快，那就又错了！\n",
    " 无语,是裙子吗?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "中文分词相对于英文来说相对困难，英文分词一般按空格切分，\n",
    " 将原始文本转换为一组标记的组合，每个标记通常是一个词。\n",
    " 而中文本身的特点决定了分词的复杂度。在此，通过比较以单\n",
    " 个词作为特征、双词作为特征、单个词和双词一起作为特征及\n",
    " 结巴分词提取特征的分类准确性，选取合适的分词方式，进行\n",
    " 后面的文本量化过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单个词作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text():\n",
    "        f1 = open('F:/课程设计及上机实验/python/文本分析/中文分词/clothing/pos.txt','r',encoding='utf-8') \n",
    "        f2 = open('F:/课程设计及上机实验/python/文本分析/中文分词/clothing/neg.txt','r',encoding='utf-8')\n",
    "        line1 = f1.readline()\n",
    "        line2 = f2.readline()\n",
    "        str = ''\n",
    "        while line1:\n",
    "            str += line1\n",
    "            line1 = f1.readline()\n",
    "        while line2:\n",
    "            str += line2\n",
    "            line2 = f2.readline()\n",
    "        f1.close()\n",
    "        f2.close()\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "     return dict([(word,True) for word in words])#返回的是字典类型,这是情感分类的一个标准形式，TRUE表示对应的元素是特征\n",
    "dc=bag_of_words(text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双个词作为特征\n",
    "把双个词作为特征，并使用卡方统计的方法，选择排名前1000的双词\n",
    "\n",
    "卡方检验原理及应用：https://blog.csdn.net/weixin_36541072/article/details/53760725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import  BigramCollocationFinder\n",
    "from nltk.metrics import  BigramAssocMeasures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  bigram(words,score_fn=BigramAssocMeasures.chi_sq,n=1000):\n",
    "        bigram_finder=BigramCollocationFinder.from_words(words)  #把文本变成双词搭配的形式\n",
    "        bigrams = bigram_finder.nbest(score_fn,n)  #使用卡方统计的方法，选择排名前1000的双词\n",
    "        newBigrams = [u+v for (u,v) in bigrams]#合并字符串u和v\n",
    "        return bag_of_words(newBigrams)\n",
    "a=(bigram(text(),score_fn=BigramAssocMeasures.chi_sq,n=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_finder=BigramCollocationFinder.from_words(text())  #把文本变成双词搭配的形式\n",
    "bigrams =bigram_finder.nbest(BigramAssocMeasures.chi_sq,20)  #使用卡方统计的方法，选择排名前1000的双词\n",
    "newBigrams = [u+v for (u,v) in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newBigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单个词和双个词一起作为特征\n",
    "update()详解：http://www.runoob.com/python/att-dictionary-update.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  bigram_words(words,score_fn=BigramAssocMeasures.chi_sq,n=1000):\n",
    "        bigram_finder=BigramCollocationFinder.from_words(words)#把文本变成双词搭配的形式\n",
    "        bigrams = bigram_finder.nbest(score_fn,n)#使用卡方统计的方法，选择排名前1000的双词\n",
    "        newBigrams = [u+v for (u,v) in bigrams]\n",
    "        a = bag_of_words(words)\n",
    "        b = bag_of_words(newBigrams)\n",
    "        a.update(b)  #把字典b合并到字典a中Python\n",
    "        return a \n",
    "print(bigram_words(text(),score_fn=BigramAssocMeasures.chi_sq,n=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
